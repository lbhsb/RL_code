# 经典强化学习算法系列

#### 此项目用于实现经典强化学习算法，首先是**DQN**系列(包括DDQN,D3QN等)，后续其他算法会陆续更新。

***

- 注：此项目代码并非原创，本人所做工作只是略做修改能在不同环境中运行并可以在tensorboard后端观察到DQN系列不同代码的reward曲线。

- 注意事项：

  - DQN,DDQN,D3QN和noisynet DQN算法都是基于pytorch实现，priority replay DQN是基于tensorflow1.实现
  - 在不同的环境(比如Atari)中实现，只需要修改env的名称。
  - 其他超参数的修改，需要手动修改
  - 每个py文件都可单独运行，若需观看不同reward曲线，只需要在当前目录下打开终端，输入***tensorboard --logdir = /logs***
  - replay_buffer里存放着不同回放函数，优先回放DQN会对之调用

  ***

  #### Q-learning 到 DQN 的演进

  虽说表格形式对于求解有很大的帮助，但它也有自己的缺点。如果问题的状态和行动的空间非常大，使用表格表示难以求解，因为我们需要将所有的状态行动价值求解出来
  Deep Q-Learning 算法简称DQN，DQN是在Q-Learning的基础上演变而来的，DQN对Q-Learning的修改主要有两个方面：

  1. **DQN利用深度神经网络逼近值函数 ：值函数是利用神经网络逼近，属于非线性逼近**
  2. **DQN利用了经验回放训练强化学习的学习过程 ：即将一个五元组 (s, a, R, s_next, is_end) 添加到一个经验池中，这些五元组之后将用来更新Q网络参数，在这里 s 和s_next 都是向量的形式，动作和奖励是标量，is_end 是布尔值**

  ***

  #### DQN 到 Nature DQN 的演进

  在计算目标值时和计算当前值用的是同一个网络Q，这样在计算目标值时用到了我们需要训练的网络Q，之后又用目标值来更新网络Q的参数，这样两者的依赖性太强，不利于算法的收敛，因此在Nature DQN中提出了使用两个网络，一个原网络Q用来选择动作，并更新参数，另一个目标网络Q′只用来计算目标值，在这里目标网络Q′的参数不会进行迭代更新，而是隔一定时间从原网络Q中复制过来。

  ***

  #### Nature DQN 到 Double DQN 的演进

  无论是DQN，还是Nature DQN都无法克服Q-Learning本身多固有的缺陷-过估计。过估计是指估计得值函数比真实值函数要大，其根源主要在于Q-Learning中的最大化操作，（注：对于真实的策略来说并在给定的状态下并不是每次都选择使得Q值最大的动作，因为一般真实的策略都是随机性策略，所以在这里目标值直接选择动作最大的Q值往往会导致目标值要高于真实值）。为了解决值函数过估计的问题，Hasselt提出了Double DQN的方法，其定义是将动作的选择和动作的评估分别用不同的值函数来实现，而在Nature DQN中正好提出了两个Q网络。Double DQN也简称DDQN，其计算目标值yj的步骤可以拆分为两步。

  1. **通过原网络Q获得最大值函数的动作a**
  2. **通过目标Q′网络获得上面的动作a对应的值**

  ***

  #### Double DQN 到 Prioritized Replay DQN 演进

  在经验回放中是利用均匀分布采样，而这种方式看上去并不高效，对于智能体而言，这些数据的重要程度并不一样，因此提出优先回放（Prioritized Replay）的方法。优先回放的基本思想就是打破均匀采样，赋予学习效率高的样本以更大的采样权重。
  一个理想的标准是智能体学习的效率越高，权重越大。符合该标准的一个选择是TD偏差δ。TD偏差越大，说明该状态处的值函数与TD目标的差距越大，智能体的更新量越大，因此该处的学习效率越高。优先回放DQN主要有两点改变：

  1. **为了方便优先回放存储与及采样，采用sumTree树来存储；**
  2. **目标函数在计算时根据样本的TD偏差添加了权重（权重和TD偏差有关，偏差越大，权重越大）**

  ***

  #### Dueling DQN 的提出

  在前面讲到的DDQN中，通过优化目标Q值的计算来优化算法，在Prioritized Replay DQN中，通过优化经验回放池按权重采样来优化算法。而在Dueling DQN中，则尝试通过优化神经网络的结构来优化算法。
  具体如何优化网络结构呢？
  Dueling DQN考虑将Q网络分成两部分，第一部分是仅仅与状态S有关，与具体要采用的动作A无关，这部分我们叫做价值函数部分，记做V(S,w,α),第二部分同时与状态状态S和动作A有关，这部分叫做优势函数(Advantage Function)部分,记为A(S,A,w,β),那么最终我们的价值函数可以重新表示为：Q(S,A,w,α,β)=V(S,w,α)+A(S,A,w,β)。其中，w是公共部分的网络参数，而α是价值函数独有部分的网络参数，而β是优势函数独有部分的网络参数。

  改进原因：

  **有些场景，环境大部分时候对Actions的响应不大，影响环境更多的是state-value V(s)。这种分离的设计方式，直觉上，能让学习更加有针对性，注意力放到关联的变量上；当action space冗余时，存在相似的actions时，对网络的扰动很小，能够让学习更有效率。**

  ***

  #### NoisynetDQN 的提出

  在强化学习中，对于探索-利用而言，目前通常采用以下两种方法：

  1. epsilon-greedy：ϵ-greedy（以超参数形式给出）很有可能会导致智能体采取随机步骤，而不是按照它学到的策略采取行动。 通常的做法是，在训练开始时使该 ϵ  = 1 ，然后慢慢减小到一个较小的值，例如0.1或0.02。
  2. 熵正则化：将策略的熵添加到损失函数中时，在策略梯度方法中使用它，以惩罚我们的模型过于确定其行为。

  这两种方式不能用于需要大规模行为模式的探索的情况。还有一类结构化的探索方法是利用额外的内在激励使探索目标更为明确，但是这种内在激励设置是人为的，而且可能会导致不稳定以及最优策略的变化。因此 NoisyNet 采用一种不同的探索模式，依赖权重空间的扰动来驱动探索。

  ***

  原理上的推导可看原论文

  DQN：https://arxiv.org/pdf/1312.5602.pdf

  Double DQN：https://arxiv.org/pdf/1509.06461.pdf

  Dueling DQN：http://arxiv.org/abs/1511.06581

  Priority Experience Replay：http://arxiv.org/abs/1511.05952

  Noisy Networks for Exploration：http://arxiv.org/abs/1706.10295

  视频教程可以参考李宏毅强化学习(https://www.bilibili.com/video/BV1UE411G78S?spm_id_from=333.999.0.0)，其他DQN系列还可参考(https://zhuanlan.zhihu.com/p/159731786)
